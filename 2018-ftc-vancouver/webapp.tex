%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{enumitem}

\begin{filecontents}{citace.bib}
@inproceedings{kruuza2012making,
  title={Making Community and ASR Join Forces in Web Environment},
  author={Kr{\uu}za, Old{\v{r}}ich and Peterek, Nino},
  booktitle={International Conference on Text, Speech and Dialogue},
  pages={415--421},
  year={2012},
  organization={Springer}
}
@article{codd1970relational,
  title={A relational model of data for large shared data banks},
  author={Codd, Edgar F},
  journal={Communications of the ACM},
  volume={13},
  number={6},
  pages={377--387},
  year={1970},
  publisher={ACM}
}
\end{filecontents}


\def\UrlFont{\rmfamily}

\begin{document}
\mainmatter              % start of a contribution
%
\title{Second-Generation Web Interface to Correcting ASR output}
%
\titlerunning{Hamiltonian Mechanics}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Old\v{r}ich Kr\r{u}za and Vladislav Kubo\v{n}}
%
\authorrunning{Kr\r{u}za, Kubo\v{n}} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
%\tocauthor{Ivar Ekeland, Roger Temam, Jeffrey Dean, David Grove,
%Craig Chambers, Kim B. Bruce, and Elisa Bertino}
%
\institute{Charles University\\
           Faculty of Mathematics and Physics\\
           Institute of Formal and Applied Linguistics\\
           Malostransk\'{e} n\'{a}m. 25, Prague, Czech Republic\\
           \{kruza,vk\}@ufal.mff.cuni.cz}

\maketitle              % typeset the title of the contribution

\begin{abstract}
This paper presents a next-generation web application that enables users to
contribute corrections to automatically acquired transcription of long speech
recordings. We describe differences from similar settings, compare our solution
with others and reflect on the development from the first, 6 years old version
in the light of the work done, lessons learned and the new technologies
available in the browser.
\keywords{Speech recognition, community-driven, web standards}
\end{abstract}

\section{Introduction}

Kr\r{u}za 2012\cite{kruuza2012making} presented a setting where a community of
users contributed corrections to automatically transcribed talks of a single
speaker. Now that the browser technologies evolved drastically and we could
observe the usage patterns and discover shortcomings of the solution at hand, we
have created a next generation of the programme. We shall describe the steps
taken and discuss their motivation and impact.

\subsection{Motivation}

Our project focuses on the collection of recordings of Karel Mako\v{n} *1912
\textdagger1993, the author of numerous books, translations and comments to
works of spiritual and religious nature, who was influenced by trances during
recurring surgery without anesthesy in the age of 6, ecstasies in the youth and
finally facing and surviving certain death in a Nazi concentration camp, after
which he experienced enlightenment. He gave talks in a narrow circle of friends
and the recordings in our care have been taken between early 70's and 1991,
spanning about 1000 hours in total.

All of his work deals more or less directly with a single topic: entering the
eternal life before the physical death. He draws mainly from the Christian
symbolism, builds up on Christian mysticism and ancient tradition of India.

Basically the meat of his teachings is systematically encompassed in his written
works, whereas the talks contain the sauce: talks tailored to the audience,
answering questions, personal experiences, behind-the-scenes to the books etc.

\section{Differences to other settings}

In our setting, we have a large spoken corpus (about 1000 hours) of a single
speaker. Our aim is to have a transcription as good as possible for the purpose
of searching and further, higher-level processing of the data. There is a pool
of people interested in the talks, who on one hand are the force we can try to
employ and on the other hand are the consumers of our effort, our target group
so to speak.

The web application should therefore combine the two purposes: 1. serve its user
with making the content available in a manner as good as possible and 2. animate
the user to give as much and as high-quality contribution as possible.

To our best knowledge, there is no other project with a comparable setting.
However, we can compare single aspects found in other applications.

\subsection{Transcription apps}

The main differences to common transcription software are that

\noindent
\begin{tabularx}{\textwidth}{
    @{\hspace{1.5em}}% Space for left bullet
    >{\leavevmode\llap{\textbullet~}\raggedright}% Left bullet + formatting of column
    X% Left column specification
    @{\quad\hspace{1.5em}}% Space between columns + right bullet space
    >{\leavevmode\llap{\textbullet~}\raggedright\arraybackslash}% Right bullet + formatting of column
    X% Right column specification
    @{}% No column space on right
  }
  \em{common transcription applications}: & \em{our application}: \\
  are optimized for the case where there is no transcription available and it
  must be acquired from scratch; &
    always assumes a transcription is available; \\
  allow annotation of speakers; &
    assumes all utterances come from the same speaker; \\
  need no quality control: the user is free to enter whatever transcription she
  pleases and the ultimate measure is her satisfaction; &
    needs the transcription to be accurate because it is used as training data
    for the acoustic model; \\
  use alignment on the level of phrases, if any; &
    uses alignment on the level of words; \\
  are user-centric: the user transcribes whatever acoustic data they choose; &
    is data-centric: the whole application with all its tools and persons
    revolves around the data set; \\
  assumes the user wants to transcribe; &
    assume the user wants to listen and possibly read along and we want to
    animate her to submit transcriptions; \\
  has no shared data between users; &
    must count with collisions.
\end{tabularx}

We can still learn a lot from transcription software. The ease of performing
common tasks, like pausing, resuming and rewinding is crucial for the user
experience and in effect for the amount of submissions that we receive. Also,
the way the text is displayed synchronously to the audio played has a big impact
and the approaches have a lot of space for variation.

\subsection{Wiki}

Where our application diverts from transciption software, it mostly resembles a
wiki: a community platform that serves its users including the contributors but
where the quality of the contributions is essential, while the contributor's
satisfaction alone is of little importance.

One major difference to a wiki is that wiki is creative, whereas our task is
mechanical. The user has basically no room for their own invention: providing a
different than correct transcription is seen as an error.

Popular wikis have good measures for edit conflicts, which is where we could
learn some lessons. However, so far there was no need to do that because
\begin{enumerate}
\item{when we always simply take the most recent version
of a segment, the result stays consistent even if a piece from user A comes into
a larger transcription of user B;}
\item{our user base is so far limited to a small community who have no problem
coordinating with each other. We plan to expand to broader public soon though.}
\end{enumerate}

\section{Description of the web application}

The application consists of several views:
\begin{enumerate}
\item{the start page where all recordings are listed and each points to a detail
view,}
\item{the detail view, where a recording can be played back, its transcription
is displayed and can be corrected by the user,}
\item{search page, where hits to a search query are listed and point to
corresponding positions in the recordings,}
\item{static pages with general information, contact etc.}
\end{enumerate}

We shall only discuss the detail view as the others are not relevant to this
article. Figure~\ref{fig:scn1lab} shows the interface during playback.
Figure~\ref{fig:scn2lab} shows the interface while a segment is being edited.
The interface in the figures is conveniently shown in English, although in
reality it is in Czech.

\begin{figure}[htpb]
\includegraphics[scale=0.6]{rc/radio-makon-en-1-lab.png}
\caption{Web interface during playback}
\label{fig:scn1lab}
\end{figure}

Legend to Figure~\ref{fig:scn1lab}:
\begin{enumerate}
\item{
    Header with
    \begin{itemize}
    \item{app name linking to start page,}
    \item{about link,}
    \item{search field and}
    \item{username input field;}
    \end{itemize}
}
\item{Identifier of the recording;}
\item{Automatically transcribed segments in gray;}
\item{Manually transcribed segments in black;}
\item{Currently played-back word highlighted by yellow background;}
\item{Marked word highlighted in regent st. blue;}
\item{
    Marked word info:
    \begin{itemize}
    \item{
        occurrence: the word with contextual capitalization and
        punctuation as it appeard in the text, currently being edited as the
        selected initial letter reveals,
    }
    \item{form: normalized word form as it appears in the word list,}
    \item{pronunciation: Czech phonetic transcription of the word,}
    \item{
        position: time of the beginning of the word in seconds from the
        start of the recording;
    }
    \end{itemize}
}
\item{
    Tools for storing:
    \begin{itemize}
    \item{direct links to the audio files,}
    \item{selecting the whole transcription for easy pasting,}
    \item{storing the decoded recording in the browser's IndexedDB;}
    \end{itemize}
}
\item{Graphical equalizer for compensating narrow-band noise;}
\item{
    Audio playback controls:
    \begin{itemize}
    \item{play/pause button,}
    \item{current playback position,}
    \item{playback scrollbar,}
    \item{total recording length;}
    \end{itemize}
}
\item{Current position reflected in URL fragment.}
\end{enumerate}

\begin{figure}[htpb]
\includegraphics[scale=0.6]{rc/radio-makon-en-2-lab.png}
\caption{Interface in the state of editing a segment, with labels}
\label{fig:scn2lab}
\end{figure}

Legend to Figure~\ref{fig:scn2lab}:
\begin{enumerate}
\item{
    Selecting a text range with the mouse defines the segment the user is about
    to transcribe;
}
\item{
    The edit tool with
    \begin{itemize}
    \item{textarea prefilled with the current transcription,}
    \item{playback button that plays the corresponding segment,}
    \item{save button and}
    \item{download-segment button, which initiates a file-save action for the
    audio segment corresponding the the selected text.}
    \end{itemize}
}
\end{enumerate}

\subsection{Displaying the transcription}

Most transcription programs show the transcription as a vertical list of
utterances, see Figure~\ref{fig:transcriber1} for an example of
{\em{Transcriber}}\footnote{trans.sourceforge.net}, a veteran open-source
transcription tool. We attribute this to the fact that the atomic elements of
the transcription are the user-entered utterances and their boundaries are
reliable. In our case, the atomic elements are words. There are sentences, sure,
but the segmentation to sentences by the ASR is very unreliable, so we want it
to be natural to transcribe a segment overlapping sentence boundaries.

\begin{figure}[htpb]
\includegraphics[scale=0.4]{rc/transcriber1.png}
\caption{A screenshot of Transcriber}
\label{fig:transcriber1}
\end{figure}

This is one of the reasons why we display the transcription basically as a
single wrapped line.

\subsubsection{Performance challenge}

The transcription display was designed to have these features:
\begin{enumerate}
\item{
    Currently played-back word should be highlighted;
    \label{feats:item:curword}
}
\item{
    Manually transcribed segments should be clearly distinct from automatically
    transcribed ones;
    \label{feats:item:manualdistinct}
}
\item{
    Selecting one or more words with the mouse should trigger transcription mode
    for the selected text;
    upon a successful save, this should be merged into the display;
    \label{feats:item:selectable}
}
\item{
    Clicking a word should bring up its context info (we call this the
    {\em marked word} as the term {\em selected word} is already taken);
    \label{feats:item:clickable}
}
\item{
    The whole transcription should be shown at once for easy searching;
    \label{feats:item:showall}
}
\item{
    The page should be responsive.\label{feats:item:speed}
}
\end{enumerate}

These requirements are harder to combine than it may seem. Notably
responsiveness is hard to combine with all of the other ones. Why is that so?

Points~\ref{feats:item:curword} through \ref{feats:item:clickable}
call for every word to be wrapped in its own element.
Point~\ref{feats:item:showall} and the median count of words in a transcript of
about 6000 yield 6000 \texttt{<span>} elements just to show the text. 

Although this may not seem like a big deal, it does make affect the
responsiveness and memory footprint of the page.

Kr\r{u}za 2012~\cite{kruuza2012making} solve this by sacrificing point~5:
% TODO obrázek MakonFM
only 3
lines of text are shown with the current word kept on the middle line. Thanks to
the development in the web standards and their support from popular browsers, a
solution is possible.

\subsubsection{Solution}

We can use the fortunate fact that manually transcribed words and automatically
transcribed ones tend to form larger chunks. The average number of words per
submitted segment is 7.9. Furthermore, the absolute majority of such segments
are adjacent to other manually transcribed chunks. 
% TODO max fragmentation
Hence, wrapping each chunk of consecutive manually or automatically transcribed
words in an HTML element is no problem, which solves
point~2.

Point~\ref{feats:item:selectable} can be implemented using
\texttt{document.selection} and the \texttt{Range} objects, which let us find
out the innermost HTML element and text offset of the start and end of the
textual selection. Since we know the length of the words, this allows us to map
the selection to the corresponding words in the transcription.

Points~\ref{feats:item:curword} and~\ref{feats:item:clickable} can be
implemented in two ways: We could either wrap the current and marked word in a
dedicated element or we could draw a highlighting rectangle beneath the word.

Wrapping the word would definetely be more robust and less error-prone but the
constant changes in the DOM during playback with possible frequent reflows speak
against it. Finding the exact position of each word and drawing a rectangle
precisely beneath it (beneath on the z-axis; over it in the x-y sense), avoiding
positioning issues and keeping the rectangle position synced even after scrolling
/ window resizing is definitely a challenge but we chose this way nonetheless.
The performance gain for the majority of the usage time outweighs the possible
errors in the corner cases, moreso since the eventual errors are not critical
and mostly remedied by further playback.

The efficiency of repositioning a rectangle is supported by the fact that we can
calculate the coordinates of all rendered words once and only recalculate them
in two cases: 1) In the rare event of screen resize and 2) when a corrected
segment is merged into the transcription, in which case we only need to
recalculate for the words further in the document.

\subsection{Implementation Details}

\subsubsection{Audio Engine}

The adoption of Web Audio API allowed for big improvements in comparison with
the original implementation described by Kr\r{u}za 2012~\cite{kruuza2012making}.
There are four major differences between using the HTML \texttt{<audio>} tag
and the Web Audio API.

\begin{itemize}
\item{It is now possible to precisely replay the selected audio span.}
\item{
    We could implement a graphical equalizer. Some recordings suffer from loud
    noise in the low frequency spectrum. A systematic approach to acoustic
    normalising of the material is a point of future work. Until that, the
    equalizer is a huge relief for the users.
}
\item{
    Thanks to the \texttt{OfflineAudioContext}, it is possible to store the
    recording in the browser's storage and avoid downloading or decoding it
    again after reload.
}
\item{
    We have also implemented saving the audio corresponding to the selected text
    segment as a sound file.
}
\end{itemize}

\subsubsection{App State Management}

We use React as the view library and Redux for state management. The good thing
about Redux is that it makes it easy to keep minimal state as the single source
of truth and everything that can be computed is computed, while avoiding
needless calculations. This is of course nothing new -- basically it is what we
know from database design as the normal representation.~\cite{codd1970relational}
It is the first time this approach reached the web front-end in such degree of
popularity though.

Also in our case, this approach makes the program more predictable, less
error-prone and, as the modern programming jargon lovingly expresses, {\em
easier to reason about}. But some of our features make this a bit complicated.

Among the states the app can enter is simple playback, inspecting a word,
transcribing a segment and replaying the selected audio span. Now, the only
relevant things we actually store is the redux state are:
\begin{enumerate}
\item{
    The array of transcription words, each of which bears the flag whether it is
    automatically or manually transcribed. This defines the manual - automatic
    chunks of words that in turn define the HTML elements wrapping them.
}
\item{
    The beginning and end of the selection in terms of chunk number and
    character offset in the chunk, which is basically what we get from the DOM
    upon a \texttt{mouseup} event.
}
\item{Whether playback is running and the current position.}
\end{enumerate}

Whether a word is marked or a segment is being edited is determined solely by
the boundaries of the selected words. If there is a selection and the beginning
and end are identical, it means a word was simply clicked and its detail is
shown (it is {\em marked}). If the boundaries span at least one character, then
all words that intersect this span are {\em selected} for correction.

Simple as it sounds, a slight problem arises when a correction is accepted and
the corrected subtitles are merged into the view. In the time after the
correction is accepted and reflected in the redux state, but before the new
chunks are rendered in the document, selection changes cannot be reliably mapped
to logical chunks. Simple null defaults solve this problem.

\section{Future Work}

We plan to focus on optimizing the app for wider audience. Experience confirms
that the Mako\v{n}'s talks are of interest to some people, and our aim is to
remove as many obstacles as possible to potentially interested people reaching
the material. The benefit from technical point of view would be clear: A web app
for listening to recordings and correcting their transcription is nice but one
that's really easy to use and inviting to people to submit corrections is nicer.

One of the aspects we want to explore is enabling people to naturally share
catching segments of talks on social networks.

Another point of near future endeavor is higher-level work with the contents. By
this we mean that we'd like to use both automatic processing methods and the
users to do semantic analysis of the talks: What topic is covered where? What
topics are covered at all? Which talks relate to which written works?, and
similar questions.

\section{Conclusion}

Our work is motivated by the will to make the most out of a specific set of
talks of spiritual nature by a single speaker. The setting and path taken are
not common and we hope to make solution that can be used by others or to inspire
others at solving their own tasks, however distantly related.

The web interface to consuming and editing audio transcription we have created
is first of its kind that we know of and we can but admire the recent
developments in the web standards, their adoption by major browsers and the
open-source JavaScript ecosystem that make this possible.

\section*{Acknowledgments}

The research was supported by SVV project number 260 453.\\
\\
use the prescribed wording: This work has been using language resources stored
and distributed  by the LINDAT/CLARIN project of the Ministry of Education,
Youth and Sports of the Czech Republic (project LM2015071).


\end{document}
