\documentclass{itatnew}
%% !!!dolezite: ak pisete po slovensky alebo po cesky pouzite
%% \documentclass[slovensky]{itatnew}
% \documentclass[cesky]{itatnew}
\usepackage{fontspec}

\begin{document}

\title{Phonetic Transcription by Untrained Annotators}

\author{Oldřich Krůza}

\institute{
    Charles University,
    Faculty of Mathematics and Physics,\\
    Institute of Formal and Applied Linguistics\\
    \email{kruza@ufal.mff.cuni.cz}
}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The paper presents an application for lay, untrained users to generate
high-quality, aligned phonetic transcription of speech. The application has been in use
for several years and has served to transcribe over 600 thousand word forms over
two versions of a web interface. We present measures for compensating the lack of expert training.
\end{abstract}

\section{Introduction}

\subsection{Our Setting}

The work presented in this paper is a part of the project that tends the spoken
corpus of Karel Mako\v{n}\cite{hajek2007cesky}. The corpus is of the single
speaker and has been recorded in amateur conditions, while the author was
speaking to his friends about a novel way to interpret the teaching of Jesus and
of mystic and spirituality in general. Karel Makoň died in 1993 and a community
of favorers of his teachings has persevered since then.

The talks can be seen as companions to Makoň's written works. Together they form
a unique, extensive, consistent systematization of the spiritual path tailored
to modern westerners and accessible primarily to Czech speakers. It draws
heavily on traditional Christian mysticism as well as ancient tradition of India
and China, adapting them for the present. The whole system can be seen as a
manual for entering the eternal life prior to the physical death.

There are over 1000 hours of digitized recordings of Karel Makoň, they are
accessible under the CC-BY license and the project aims at bringing the most
benefit out of them. The first step was digitizing the recordings from the
original magnetic tapes, the second step was releasing all of them on the
world-wide web, the third step was developing a web-based system for human /
machine transcription of the bulk, allowing for search.

The transcription we do is both phonetic and orthographic.\footnote{There is no
actual focus on orthography. Instead, we mean the natural way of transcribing the
speech to human-readable text. Where it matters, focus is directed at precise
correspondence with the utterances instead of language cleanliness.} Our users
are supposed provide orthographic transcription where the pronunciation is
standard and phonetic otherwise.

\subsection{Architecture Overview}

The system consists of

\begin{enumerate}
\item{
    The corpus in compressed audio format. We use mp3 and ogg/vorbis to
    accomodate most browsers.  These data are hosted on an external CDN.
}
\item{
    The exact copy of the corpus in parametrized (MFCC) format. These data
    reside on the back-end server.
}
\item{
    A complete, aligned transcription of the recordings, hosted on the back-end
    server and mirrored on a CDN.
}
\item{
    Acoustic model trained on the human-transcribed part of the corpus.
}
\item{
    Language model trained using Srilm\cite{stolcke2002srilm} on a combination
    of publicly available Czech texts, Karel Makoň's written works, and both the
    human-submitted and automatically-acquired transcription.
}
\item{
    Back-end API for collecting correcions to the transcription, serving the
    transcription and allowing full-text search with
    elasticsearch\footnote{https://www.elastic.co/products/elasticsearch}.
}
\item{
    Separately hosted front-end web application serving as an interface for
    playing the recordings, synchronously displaying the transcriptions and
    collecting the corrections from users.
}
\end{enumerate}

To get the initial transcription, we have manually transcribed some 10 minutes
of the material using Transcriber\footnote{http://trans.sourceforge.net/},
trained an acoustic model on it and recognized the whole data using it.

\section{Annotator Expertise}

Our case is on the edge of what can be called linguistic data annotation. In our
lucky part of the world where alphabetization nears 100\%, transcription of
speech is hardly expert work. On the other hand, ensuring that the transcription
exactly matches the audio
\begin{itemize}
\item{as a representation of the words uttered and of their meaning,}
\item{on the phonetic level, phone for phoneme,\footnote{In the sense that each
written phoneme corresponds to exactly one uttered phone.}}
\item{on the time axis}
\end{itemize}
is beyond what can be expected from an untrained user.

Linguistic data annotation in general requires trained personnel. If we only
look at the Prague Dependency Treebank, we can notice the annotators provided
such a degree of expertise they have become the
co-authors\cite{hajivc2005complex}.

Crowdsourcing, community-driven approach or engaging volunteers is an ever
stronger, popular way of obtaining assets that would otherwise be unbearably
costly. Let us mention for example Mihalcea (2004)\cite{mihalcea2004building}
who delegates word-sense disambiguation to volunteers. The
Wikicorpus\cite{reese2010wikicorpus} as well as the MASC\cite{ide2010manually}
gather annotation from volunteers.

In most cases, quality is very important for data annotation, so some kind of
control is essential, no matter how expert the annotators. Trivially, the less
expertise, the more control is needed.

\subsection{Quality Control}

A common way of dealing with quality control is to inspect annotator agreement.
This has the huge downside that every piece of data must be annotated at least
twice, which reduces the yield by 50+\%.

There is another reason not to use it in our case. Our application is
designed for people who want to listen to the recordings out of interest and
their contribution to the quality of the transcription is more of a by-product.
It would be hard to convince them to choose exactly a recording that another
user has already transcribed.

Luckily, we can implement automatic measures to aid the annotators to deliver
higher-quality transcription.

\subsection{Forced Alignment}

We always assume an existing transcription, so we can see the user's
contribution as a correction. Every submission has the form of replacing a text
segment with another. Since the transcriptions are time-aligned to the audio, we
also know exactly what is the corresponding audio segment to the text submitted.

This enables us to perform forced alignment on the submitted text and the audio.
With a well selected pruning threshold, we can distinguish false transcriptions
and reject them, providing feedback to the contibutor.
Since every segment of audio fits the acoustic model to a different degree, both
false positives and false negatives will inevitably occur.

False positives (when the system accepts a wrong transcription) present a
problem, since the error will enter the training data set. But users can often
circumvent false negatives by submitting the transcription divided in different
segments.  Of course, this method can also be used to force a wrong
transcription but we assume no malevolence on the part of the users.

Apart from catching wrong transcription, the forced alignment mechanism provides
exact synchronization on the time axis. This is a completely missing element in
the case of virtually all programs for computer-aided transcription. For some
examples, Transcriber, a veteran open-source transcribing program for Linux,
expects the user to provide alignment on the level of phrases;
Transcribe,\footnote{https://transcribe.wreally.com/} a commercial web-based
transcribing tool, allows the user to add timestamps anywhere in the text. There
is no acoustic model, hence nothing to match against.

\section{Phonetic Transcription}

\subsection{Purpose}

We have originally built the acoustic model using
HTK,\footnote{http://htk.eng.cam.ac.uk/} the Hidden Markov Model Toolkit. Here,
explicit phonetically labeled training data are necessary for training. We are
switching to DNN, using Mozilla's
DeepSpeech,\footnote{https://github.com/mozilla/DeepSpeech} where no explicit
phonetic annotation is needed but for some purposes like forced alignment, the
original HMM is still irreplaceable.

Also, the phonetic labeling is valuable per se for research purposes.

\subsection{Phoneme Set}

We use a subset of PACal\cite{nouza1997phonetic}. We shall also refer to
individual phonemes in this paper using the PACal notation in \texttt{monospace
font}. For reference, Table~\ref{tab:phones} lists the phonemes used with their
IPA notation.

\begin{table*}[htpb]
\fontspec{DoulosSIL}
\begin{center}
\begin{tabular}{|l|l|l||l|l|l|}
\hline
IPA & PACal & common grapheme & IPA & PACal &  common grapheme \\
\hline
% TODO: IPA
a  & a   & a      &     ɱ  & mg  & tra\underline{m}vaj \\
aː & aa  & á      &     n  & n   & \underline{n}e \\
aʊ̯ & aw  & au     &     ŋ  & ng  & ta\underline{n}k \\
b  & b   & b      &     ɲ  & nj  & \v{n} \\
t͡s & c   & c      &     o  & o   & o \\
t͡ʃ & ch  & č      &     oː & oo  & ó \\
d  & d   & d      &     oʊ̯ & ow  & ou \\
ɟ  & dj  & \v{d}  &     p  & p   & p \\
d͡z & dz  & dz     &     r  & r   & r \\
d͡ʒ & dzh & dž     &     r̝̊  & rsh & t\underline{\v{r}}i \\
ɛ  & e   & e      &     r̝  & rzh & \underline{\v{r}}íz \\
ɛː & ee  & é      &     s  & s   & s \\
eʊ̯ & ew  & eu     &     ʃ  & sh  & š \\
f  & f   & f      &     t  & t   & t \\
g  & g   & g      &     c  & tj  & \v{t} \\
ɦ  & h   & h      &     ʊ  & u   & u \\
i  & i   & i      &     uː & uu  & ú, \r{u} \\
iː & ii  & í      &     v  & v   & v \\
j  & j   & j      &     x  & x   & ch \\
k  & k   & k      &     z  & z   & z \\
l  & l   & l      &     ʒ  & zh  & ž \\
m  & m   & \underline{m}ák
                  &        & sil & \\
   &     &        &        & sp  & \\
\hline
\end{tabular}
\caption{Phonemes used in transcription}\label{tab:phones}
\end{center}
\end{table*}
\normalfont

\subsection{Acquisition}

The phonetic transcription is in normal case also a product of forced alignment,
as in case of pronunciation variants, it selects the most fitting one. This
requires a way to automatically obtain all pronunciation variants of any word.
We use a combination of a rule-based system inspired by Psutka et
al.\cite{psutka2004development}, in combination with a dynamic dictionary.
The dynamic dictionary is a list of alternative pronunciations of a word, which
expands as the app is being used.

The users are instructed to transcribe any words with non-standard pronunciation
phonetically and then correct their orthographical form. This is one of the few
cases where we are coercing the users to something.

When the orthographically broken, phonetic transcription of a word is submitted,
if it passes the forced-alignment phase, it is integrated into the displayed
transcription. The word's data representation consists of its
\begin{enumerate}
\item{occurrence:
    the word as it appears in the text, including capitalization and
    punctuation,
}
\item{wordform:
    the word as it appears in the language model and phonetic dictionary
    (computed as the occurrence in lowercase and stripped of non-alphabetic
    characters\footnote{This implies that all non-alphabetic characters are
    always a part of a token and never form a token on their own.}),
}
\item{pronunciation:
    an array of phonemes,
}
\item{timestamp:
    distance of the beginning of the word from the beginning of the file, in
    seconds, in precision of 2 decimal digits,
}
\item{manual/automatic:
    boolean flag denoting whether the word has been transcribed manually or not,
}
\item{confidence measure:
    in case of automatically acquired words, the confidence-measure score of the
    recognizer.
}
\end{enumerate}
Once merged into the displayed transcription, each word's occurrence can be
edited manually. Now the user can enter the correct form deviating from Czech
pronunciation rules.

Doing so results in adding the wordform-pronunciation couple to the dynamic
pronunciation dictionary and is also used for forced alignment. Thus, this
operation need only be performed once per word and any subsequent time the word
is entered in its standard orthographic form, the correct pronunciation is
inferred.

For example, let's examine the scenario of transcribing the sentence {\em Proč
se toto nestalo Marii Markétě Alacoque? (Why hasn't this happened to Mary
Margaret Alacoque?)} Its phonetic representation is \texttt{
    p r o ch sp
    s e sp
    t o t o sp
    n e s t a l o sp
    m a r i j i sp
    m a r k ee tj e sp
    a l a k o k sil
}.
\begin{enumerate}
\item{Suppose the user enters the correct ortographic transcription.}
\item{
    The phonetic transducer outputs \texttt{
        p r o ch sp
        s e sp
        t o t o sp
        n e s t a l o sp
        m a r i j i sp
        m a r k ee tj e sp
        a l a c o k v u e sil
    }.
}
\item{
    With a bit of luck, the forced alignment fails because of the distiction of
    the phone sequence\\ \texttt{k o k} and \texttt{c o k v u e}.
}
\item{
    The transcription is rejected, the user realizes that the word is pronounced
    in a non-standard way and re-tries with
    {\em Proč se toto nestalo Marii Markétě alakok?}
}
\item{
    Forced alignment succeeds now and the entered transcription is merged into
    the view.
}
\item{
    The user selects the non-existent word {\em alakok?} and edits its occurrence
    to {\em Alacoque?}
}
\item{
    Now the word is correctly stored and on any subsequent user inputs of {\em
    Alacoque} with any punctuation or capitalization, the pronunciation
    \texttt{a l a k o k} is inferred by the forced alignment.
}
\end{enumerate}

\subsection{Phonetic Respelling}
\label{subsec:respelling}

With all advantages of using PACal as a representation for phonemes, it is
clearly not the most natural way for lay Czechs to write down and read literal
pronunciation. Thanks to the simple, mostly
deterministic mapping between phonemes and graphemes, pronunciation respelling
is a reliable, natural way. There's not even a need for explicit syllable
separation as seen in English pronunciation respelling
(wikipedia\footnote{https://en.wikipedia.org/wiki/Pronunciation\_respelling}
gives the example {\em ``Diarrhoea'' is pronounced DYE-uh-REE-a}).
We postulate that the phonetic respelling is natural to all alphabetized native
Czech speakers as a fact without any supporting research, based on experience
alone. 

The previous subsection gave an example of using pronunciation respelling in
Czech with the example of {\em alakok} for {\em Alacoque}. The direction from
the phonetic respelling to the phoneme array is covered by the
ortographic-to-phonetic transducer. But we also need the opposite direction to
provide the users a way to check whether the pronunciation selected by the
forced alignment fits.

For this purpose, we have created a JavaScript module for transduction between
the array of phonemes and the pronunciation
respelling.\footnote{https://github.com/Sixtease/MakonReact/\allowbreak{}blob/master/src/lib/Phonet.js}

The algorithm is simple. In most cases, a phoneme corresponds uniquely to one
character in the respelling. Exceptions are as follows:
\begin{enumerate}
\item{The phoneme \texttt{x} is spelled {\em ch}.}
\item{The phonemes \texttt{dz dzh} are spelled {\em dz dž}.}
\item{The diphtongs \texttt{aw ew ow} are spelled {\em au eu ou}.}
\item{
    Sequences \texttt{c h, o u, a u, e u, d z, d zh} are
    spelled {\em c'h, o'u, a'u, e'u, d'z, d'ž}. Note
    though, that the sequence \texttt{c h} is purely hypothetical, as it
    contradicts voiced/voiceless assimilation.
}
\item{
    Voiceless alveolar fricative trill is explicated as {\em r'}.
}
\item{
    Palatal nasal and labiodental nasal are spelled {\em n', m'}.
}
\item{Trailing silence is not represented.}
\end{enumerate}

The module includes two-way transduction, although only the one from array of
phonemes to human-readable phonetic respelling is needed in our application.
Still, the user can mark up special-case pronunciation with the apostrophe, like
the sequence of phonemes \texttt{o} and \texttt{u} with the string \texttt{o'u}.
The need has never occurred during the six years' lifespan of the application.

Note that when encoding into the phonetic respelling, none of
{\em di ti ni dě tě ně} is ever output. The palatal consonants are always
explicitly spelled out and e.g. the sequence \texttt{n i} is always spelled
{\em ny}

A few examples of words, pronunciation and phonetic respelling as output by the
algorithm (given the corresponding pronunciation is input as phoneme list):
\begin{itemize}
\item{nic /\texttt{nj i c}/: {\em ňic},}
\item{kdo /\texttt{g d o}/: {\em gdo},}
\item{disk /\texttt{d i s k}/: {\em dysk},}
\item{dřít /\texttt{d rzh ii t}/: {\em dřít},}
\item{třít /\texttt{t rsh ii t}/: {\em tř'ít},}
\item{auto /\texttt{aw t o}/: {\em auto},}
\item{nauka /\texttt{n a u k a}/: {\em na'uka},}
\item{džbán /\texttt{dzh b aa n}/: {\em džbán},}
\item{odžít /\texttt{o d dz ii t}/: {\em od'žít},}
\item{odznak /\texttt{o dz n a k}/: {\em odznak},}
\item{podzemí /\texttt{p o d z e m ii}/: {\em pod'zemí},}
\item{noc /\texttt{n o c}/: {\em noc},}
\item{tento /\texttt{t e n t o}/: {\em tento},}
\item{hangár /\texttt{h a ng g aa r}/: {\em han'gár},}
\item{samba /\texttt{s a m b a}/: {\em samba},}
\item{tonfa /\texttt{t o mg f a}/: {\em tom'fa}.}
\end{itemize}

The use of apostrophe for distinguishing ambiguities and special cases is
not 100\% intuitive and presents another point where instruction is necessary
for the user to use this feature properly.

\section{Evaluation}

We have presented our web application as a tool that enables gathering precisely
aligned, phoneme-exact transcription from untrained casual visitors. We have
presented measures for reaching this goal but the degree to which it was reached
remains unclear.

We have no gold standard data to measure the quality of our manual
transcriptions. On the contrary, we use the manual transcriptions as gold
standard for the automatic recognition. What we can to, however, is look at some
random samples and try to get a rough idea of how the system performs.

\subsection{Validation by Forced Alignment}

One thing we can examine are the approvals / rejections of the forced alignment.
Of 109640 forced alignment attempts, 3419 have failed, which makes for 3.12\%
rejection rate.  We have manually inspected 20 random failed attempts and came
to the following numbers:
\begin{itemize}
\item{
    11 cases were false negatives, where the transcription was correct and
    should have been accepted,
}
\item{
    4 cases were caused by acoustic irregularities like noise,
}
\item{
    4 cases were true negatives caused by wrongly chosen segment boundaries and
}
\item{
    1 case was true negative caused by wrong transcription.
}
\end{itemize}

Hence, in 25\% of the minimalistic sample, the forced alignment did its job of a
validator and prevented a piece of broken training data from entering the
dataset. In 55\% it was a nuisance and failure, and in the remaining 20\%, it
rejected a valid transcription but prevented a bad training example from
occurring, so we can see this in positive light.

\subsection{Non-Standard Pronunciation}

We can also track how the scenario described in
subsection~\ref{subsec:respelling} is applied. We have looked up four promising
example records in the dynamic dictionary and checked submitted transcriptions
containing them. Table~\ref{tab:pronunc} lists for each of them the correct
orthographic form, the wrong pronunciation obtained by the transducer, the
correct pronunciation and finally the phonetic respelling. Each is followed by
the number of occurrences in the manually transcribed data.

\begin{table*}[htpb]
\begin{center}
\begin{tabular}{|l r|l r|l r|l r|}
\hline
Correct spelling            & \#
	& wrong phonetic pronunc.   & \#
		& correct pronunciation     & \#
			& phon. respel.             & \# \\
\hline
Moody & 2 & \texttt{m o o d i} & 0 & \texttt{m uu d i} & 4 & {\em múdy}, {\em můdy} & 2 \\
Descartes & 2 & \texttt{d e s c a r t e s} & 0 & \texttt{d e k aa r t} & 4 & {\em dekárt} & 2   \\
Weinfurter & 30 & \texttt{v e j n f u r t e r} & 13 & \texttt{v a j n f u r t r} & 19 & {\em vajnfurtr} & 2 \\
Michelangelo & 6 & \texttt{m i x e l a ng g e l o} & 2 & \texttt{m i k e l a n dzh e l o} & 4 & {\em mikelandželo} & 0 \\
\hline
\end{tabular}
\caption{Examples of non-standard pronunciation in the manually transcribed data}
\label{tab:pronunc}
\end{center}
\end{table*}

\begin{table*}[htpb]
\begin{center}
\begin{tabular}{|l|r|r|}
\hline
 & phonetically correct & phonetically incorrect \\
\hline
orthographically correct & 25 & 15 \\
\hline
orthographically incorrect & 6 & 0 \\
\hline
\end{tabular}
\caption{Success rate for phonetic and orthographic representation of foreign
words based on data from table\ref{tab:pronunc}}
\label{tab:pronunc-rate}
\end{center}
\end{table*}

We can see in Table~\ref{tab:pronunc-rate} that the majority of cases results in
both orthographic and phonetic forms being correct. Only in about 13\% cases,
the orthographically incorrect form is kept. We attribute this to the fact that
those who use the phonetic respelling are aware of the problematic and mostly
go the whole way and clean up.

On the other hand, nearly a third of the cases show the wrong phonetic
representation. This is a serious problem on at least two levels: Firstly, it
shows that the forced aligner failed to catch the error. Secondly, it lets bad
examples into the training dataset.

One of the apparent reasons for this to happen is that the dynamic dictionary
only recognizes exact matches. We can see in one file, for example, all
occurrences of the form {\em Weinfurter} to have correct pronunciation while
{\em Weinfurterovi} to have a broken one.

Other factors likely include user carelessness or ignorance, which is
exactly what our application is trying to compensate, but fails in these cases.

The cases with false orthographic form don't pose much of a problem. It can
harden searching for the term in question but performing a search for the
phonetic respelling or even automatically searching the pronunciation would
easily mitigate this.

The fourth combination of phonetic respelling and false pronunciation is of
course not occuring.

\section{Conclusion}

We have presented an application that has been providing access to the
extensive corpus of Karel Makoň and to acquire an almost complete transcription
thereof. Nearly 70 hours corresponding to over 600,000 word forms have been
transcribed manually with minimal financial\footnote{In early stages, we kept a paid
annotator to test the application.} as well as development\footnote{The system
has been written by a single developer.} costs. Only some of the volunteers have
indulged instruction time in order of minutes. The rest of the corpus has been
transcribed using an ASR system trained on these ever-growing data.

We have presented the ways we use to aid the untrained users to provide a
high-quality orthographic and phonetic time-aligned transcription. We have
attempted a rough evaluation of the success rate of the measures presented.
Though clearly far from perfect, they do serve the purpose and set a baseline
for improvements or novel approaches.

The system has been built with the motivation of spreading the message contained
in Karel Makoň's talks. However, to make the technology more useful, we are
actively looking for similar settings where it could be deployed.

\section*{Acknowledgments}

The research was supported by SVV project number 260 453.\\
\\
This work has been using language resources stored
and distributed  by the LINDAT/CLARIN project of the Ministry of Education,
Youth and Sports of the Czech Republic (project LM2015071).

\bibliographystyle{unsrt}
\bibliography{citace}

\end{document}
