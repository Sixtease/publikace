\documentclass{itatnew}
%% !!!dolezite: ak pisete po slovensky alebo po cesky pouzite
%% \documentclass[slovensky]{itatnew}
%% \documentclass[cesky]{itatnew}

\begin{document}

\title{Phonetic Transcription by Untrained Annotators}

\author{Oldřich Krůza\inst{1}}

\institute{ÚFAL MFF UK,\\
\email{kruza@ufal.mff.cuni.cz}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The paper presents an application for lay, untrained users to generate
high-quality, aligned phonetic transcription of speech. The application has been in use
for several years and has served to transcribe over 600 thousand word forms over
two versions of a web interface. We present measures for compensating the lack of expert training.
\end{abstract}

\section{Introduction}

\subsection{Our Setting}

The work presented in this paper is a part of the project that tends the spoken
corpus of Karel Mako\v{n}\cite{hajek2007cesky}. The corpus is of the single
speaker and has been recorded in amateur conditions, while the author was
speaking to his friends about a novel way to interpret the teaching of Jesus and
of mystic and spirituality in general. Karel Makoň died in 1993 and a community
of favorers of his teachings has persevered since then.

There are over 1000 hours of digitized recordings of Karel Makoň, they are
accessible under the CC-BY license and the project aims at bringing the most
benefit from them. The first step was digitizing the recordings from the
original magnetic tapes, the second step was releasing all of them on the
world-wide web, the third step was developing a web-based system for human /
machine transcription of the bulk, allowing for search.

The transcription we do is both phonetic and orthographic\footnote{There is no
actual focus on orthography. Instead, we mean the natural way of transcribing the
speech to human-readable text. Where it matters, focus is directed at precise
correspondence with the utterances instead of language cleanliness}. Our users
are supposed provide orthographic transcription where the pronunciation is
standard and phonetic otherwise.

\section{Annotator Expertise}

Our case is on the edge of what can be called linguistic data annotation. In our
lucky part of the world where alphabetization nears 100\%, transcription of
speech is hardly expert work. On the other hand, ensuring that the transcription
exactly matches the audio
\begin{itemize}
\item{as a representation of the words uttered and their meaning,}
\item{on the phonetic level, phoneme for phoneme,}
\item{on the time axis}
\end{itemize}
is beyond what can be expected from an untrained user.

Linguistic data annotation in general requires trained personnel. If we only
look at the Prague Dependency Treebank, we can notice the annotators provided
such a degree of expertise they have become the
co-authors\cite{hajivc2005complex}.

Crowdsourcing, community-driven approach or engaging volunteers is an ever
stronger, popular way of obtaining assets that would otherwise be unbearably
costly. Let us mention for example Mihalcea (2004)\cite{mihalcea2004building}
who delegates word-sense disambiguation to volunteers. The
Wikicorpus\cite{reese2010wikicorpus} as well as the MASC\cite{ide2010manually}
gather annotation from volunteers.

In most cases, quality is very important for data annotation, so some kind of
control is essential, no matter how expert the annotators. Trivially, the less
expertise, the more control is needed.

\subsection{Quality Control}

A common way of dealing with quality control is to inspect annotator agreement.
This has the huge downside that every piece of data must be annotated at least
twice, which reduces the yield by 50+\%.

There is another reason not to use it in our case. Our application is
designed for people who want to listen to the recordings out of interest and
their contribution to the quality of the transcription is more of a by-product.
It would be hard to convince them to choose exactly a recording that another
user has already transcribed.

Luckily, we can implement automatic measures to aid the annotators to deliver
higher-quality transcriptions.

\subsection{Forced Alignment}

Since we always assume an existing transcription, we can see the user's
contribution as a correction. Every submission has
the form of replacing a text segment with another. Since the transcriptions are
time-aligned to the audio, we also know exactly what is the corresponding audio
segment to the text submitted.

This enables us to perform forced alignment on the submitted text and the audio.
With a well selected pruning threshold, we can distinguish false transcriptions
and reject them, providing feedback to the contibutor.
Since every segment of audio fits the acoustic model to a different degree, both
false positives and false negatives will inevitably occur.

False positives (when the system accepts a wrong transcription) present a
problem, since the error will enter the data set. But false negatives can often
be circumvented by submitting the transcription divided in different segments.
Of course, this method can also be used to force a wrong transcription but we
assume no malevolence on the part of the users.

Apart from catching wrong transcription, the forced alignment mechanism provides
exact synchronization on the time axis. This is a completely missing element in
the case of virtually all programs for computer-aided transcription. For some
examples, Transcriber\footnote{http://trans.sourceforge.net/}, a veteran
open-source transcribing program for Linux, expects the user to provide
alignment on the level of phrases;
Transcribe\footnote{https://transcribe.wreally.com/}, a commercial web-based
transcribing tool, allows the user to add timestamps anywhere in the text. There
is no acoustic model, hence nothing to match against.

\section{Phonetic Transcription}

\subsection{Purpose}

We have originally built the acoustic model using
HTK\footnote{http://htk.eng.cam.ac.uk/}, the Hidden Markov Toolkit. Here,
explicit phonetically labeled training data are necessary for training. We are
switching to DNN, using Mozilla's
DeepSpeech\footnote{https://github.com/mozilla/DeepSpeech}, where no explicit
phonetic annotation is needed but for some purposes like forced alignment, the
original HMM is still irreplaceable.

Also, the phonetic labeling is valuable per se for research purposes.

\subsection{Acquisition}

The phonetic transcription is in normal case also a product of forced alignment,
as in case of pronunciation variants, it selects the most fitting one. This
requires a way to automatically obtain all pronunciation variants of any word.
We use a combination of a rule-based system inspired by Psutka et
al.\cite{psutka2004development}, in combination with a dynamic dictionary.
The dynamic dictionary is a list of alternative pronunciations of a word, which
expands as the app is being used.

The users are instructed to transcribe any words with non-standard pronunciation
phonetically and then correct their orthographical form. This is one of the few
cases where we are coercing the users to something.

When the orthographically broken, phonetic transcription of a word is submitted,
if it passes the forced-alignment phase, it is integrated into the displayed
transcription. The word's data representation consists of its
\begin{enumerate}
\item{occurrence:
    the word as it appears in the text, including capitalization and
    punctuation,
}
\item{wordform:
    the word as it appears in the language model and phonetic dictionary
    (computed as the occurrence in lowercase and stripped of non-alphabetic
    characters\footnote{This implies that all non-alphabetic characters are
    always a part of a token and never form a token on their own.}),
}
\item{pronunciation:
    an array of phonemes,
}
\item{timestamp:
    distance of the beginning of the word from the beginning of the file, in
    seconds, in precision of 2 decimal digits,
}
\item{manual/automatic:
    boolean flag denoting whether the word has been transcribed manually or not,
}
\item{confidence measure:
    in case of automatically acquired words, the confidence-measure score of the
    recognizer.
}
\end{enumerate}
Once merged into the displayed transcription, each word's occurrence can be
edited manually. Hence the user can enter the correct form deviating from Czech
pronunciation rules.

Doing so results in adding the wordform-pronunciation couple to the dynamic
pronunciation dictionary and is also used at forced alignment. Thus, this
operation must only be performed once per word and any subsequent time the word
is entered in its standard orthographic form, the correct pronunciation is
inferred.

For example, let's examine the scenario of transcribing the sentence {\em Proč
se toto nestalo Marii Markétě Alacoque?} Its phonetic representation is {\em p r
o ch sil s e sil t o t o sil n e s t a l o sil m a r i j i sil m a r k ee tj e
sil a l a k o k}.
\begin{enumerate}
\item{Suppose the user enters the correct ortographic transcription.}
\item{
    The phonetic transducer outputs {\em
        p r o ch sil
        s e sil
        t o t o sil
        n e s t a l o sil
        m a r i j i sil
        m a r k ee tj e sil
        a l a c o k v u e
    }.
}
\item{
    With a bit of luck, the forced alignment fails because of the distiction of
    the phone sequence {\em k o k} and {\em c o k v u e}.
}
\item{
    The transcription is rejected, the user realizes that the word is pronounced
    in a non-standard way and re-tries with
    {\em Proč se toto nestalo Marii Markétě alakok?}
}
\item{
    Forced alignment succeeds now and the entered transcription is merged into
    the view.
}
\item{
    The user selects the non-existent word {\em alakok} and edits its occurrence
    to {\em Alacoque?}
}
\item{
    Now the word is correctly stored and on any subsequent user inputs of {\em
    Alacoque} with any punctuation or capitalization, the pronunciation
    {\em a l a k o k} is considered at the forced alignment.
}
\end{enumerate}

\subsection{Phonetic Respelling}
\label{subsec:respelling}

As can be seen above, for the sake of simplicity and interoperability, we use
the set of phonemes as defined by Nouza et al.\cite{nouza1997phonetic} With all
advantages of this representation, it is clearly not the most natural way for
lay Czechs to write down literal pronunciation. Thanks to the simple, mostly
deterministic mapping between phonemes and graphemes, pronunciation respelling
is a reliable, natural way. There's not even a need for explicit syllable
separation as seen in English pronunciation respelling {\em
(wikipedia\footnote{https://en.wikipedia.org/wiki/Pronunciation\_respelling}
gives the example {\em ``Diarrhoea'' is pronounced DYE-uh-REE-a})}.

The previous subsection gave an example of using pronunciation respelling in
Czech with the example of {\em alakok} for {\em Alacoque}. The direction from
the phonetic respelling to the phoneme array is covered by the
ortographic-to-phonetic transducer. But we also need the opposite direction to
provide the users a way to check whether the pronunciation selected by the
forced alignment fits.

For this purpose, we have created a JavaScript module for transduction between
the array of phonemes and the pronunciation
respelling\footnote{https://github.com/Sixtease/MakonReact/blob/master/src/lib/Phonet.js}.

The algorithm is simple. In most cases, a phoneme corresponds uniquely to one
character in the respelling. Exceptions are as follows:
\begin{enumerate}
\item{The phone {\em (x)} is spelled {\em ``ch''}.}
\item{The phone {\em (dz) (dzh)} are spelled {\em ``dz'' ``dž''}.}
\item{The diphtongs {\em (aw) (ew) (ow)} are spelled {\em ``au'' ``eu'' ``ou''}.}
\item{
    Sequences {\em (c) (h), (o) (u), (a) (u), (e) (u), (d) (z), (d zh) } are
    spelled {\em ``c'h'', ``o'u'', ``a'u'', ``e'u'', ``d'z'', ``d'ž''}. Note
    though, that the sequence {\em (c) (h)} is purely hypothetical, as it
    contradicts voiced/voiceless assimilation.
}
\item{
    Voiceless alveolar fricative trill is explicated as {\em ``r'{}''} as in {\em
    tř'ít} vs. {\em dřít}.
}
\item{
    Palatal nasal and labiodental nasal are spelled {\em ``n'{}'', ``m'{}''}.
}
\item{Trailing silence is not represented.}
\end{enumerate}

The module includes two-way transduction, although only the one from array of
phonemes to human-readable phonetic respelling is needed in our application.
Still, the user can mark up special-case pronunciation with the apostrophe, like
the sequence of phonemes {\em (c)} and {\em (h)} with the string {\em c'h}. The
need has never occurred during the six years' lifespan of the application.

Note that when encoding into the phonetic respelling, none of
{\em di ti ni dě tě ně} is ever output. The palatal consonants are always
explicitly spelled out and e.g. the sequence {\em (n) (i)} is always spelled
{\em``ny''}

A few examples of words and their phonetic respelling as output by the
algorithm (given the corresponding pronunciation is input as phoneme list):
\begin{itemize}
\item{nic: {\em ňic},}
\item{kdo: {\em gdo},}
\item{disk: {\em dysk},}
\item{dřít: {\em dřít},}
\item{třít: {\em tř'ít},}
\item{auto: {\em auto},}
\item{nauka: {\em na'uka},}
\item{džbán: {\em džbán},}
\item{odželezněný: {\em od'železněný},}
\item{odznak: {\em odznak},}
\item{podzemí: {\em pod'zemí},}
\item{noc: {\em noc},}
\item{tento: {\em tento},}
\item{hangár: {\em han'gár},}
\item{samba: {\em samba},}
\item{tonfa: {\em tom'fa}.}
\end{itemize}

We postulate that the phonetic respelling is natural to all alphabetized native
Czech speakers as a fact without any supporting research, based on experience
alone. The use of apostrophe for distinguishing ambiguities and special cases is
not 100\% intuitive and presents another point where instruction is necessary
for the user to use this feature properly.

\section{Evaluation}

We have presented our web application as a tool that enables gathering precisely
aligned, phoneme-exact transcription from untrained casual visitors. We have
presented measures for reaching this goal but the degree to which it was reached
remains unclear.

We have no gold standard data to measure the quality of our manual
transcriptions. On the contrary, we use the manual transcriptions as gold
standard for the automatic recognition. What we can to, however, is look at some
random samples and try to get a rough idea of how the system performs.

\subsection{Validation by Forced Alignment}

One thing we can examine are the approvals / rejections of the forced alignment.
Of 109640 forced alignment attempts, 3419 have failed, which makes for 3.12\%.
We have manually inspected 20 random failed attempts and came to the following
numbers:
\begin{itemize}
\item{
    11 cases were false negatives, where the transcription was correct and
    should have been accepted,
}
\item{
    4 cases were caused by acoustic irregularities like noise,
}
\item{
    4 cases were true negatives caused by wrongly chosen segment boundaries and
}
\item{
    1 case was true negative caused by wrong transcription.
}
\end{itemize}

Hence, in 25\% of the minimalistic sample, the forced alignment did its job of a
validator and prevented a piece of broken training data from entering the
dataset. In 55\% it was a nuisance and failure, and in the remaining 20\%, it
rejected a valid transcription but prevented a bad training example from
occurring, so we can see this in positive light.

\subsection{Non-Standard Pronunciation}

We can also track how the scenario described in
subsection~\ref{subsec:respelling} is applied. We have looked up four promising
example records in the dynamic dictionary and checked submitted transcriptions
containing them. Table~\ref{tab:pronunc}

\begin{table}[htpb]
\begin{center}
\begin{tabular}{|l r|l r|l r|l r|}
\hline
Correct written form        & \#
	& wrong phonetic pronunc.   & \#
		& correct pronunc.          & \#
			& phon. respelling          & \# \\
\hline
Moody & 2 & {\texttt m o o d i} & 0 & {\texttt m uu d i} & 4 & {\em múdy}, {\em můdy} & 2 \\
Descartes & 2 & {\texttt d e s c a r t e s} & 0 & {\texttt d e k aa r t} & 4 & {\em dekárt} & 2   \\
Weinfurter & 18 & {\texttt v e j n f u r t e r} & 9 & {\texttt v a j n f u r t r} & 16 & {\em vajnfurtr} & 9 \\
Michelangelo & 6 & {\texttt m i x e l a ng g e l o} & 9 & {\texttt m i k e l a n dzh e l o} & 9 & {\em mikelan'dželo} & 4 \\
\hline
\end{tabular}
\caption{Examples of non-standard pronunciation in the manually transcribed data}\label{tab:pronunc}
\end{center}
\end{table}



\end{document}
